\documentclass[10pt,landscape, a4paper]{article}
\usepackage[legalpaper, landscape, right=.2in, top=.25in, bottom=.5in, left=.2in]{geometry}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tcolorbox}
\setlength{\columnseprule}{1pt}
\setcounter{section}{1}
\tcbuselibrary{listings,breakable}


\newcommand{\custombox}[3]{\begin{tcolorbox}[left=0mm,right=0mm,bottom=0mm,top=0mm,title = \textbf{#1}, colback=#2!10!white, colframe = #2!70!white, coltitle=white, breakable]
    #3
    \end{tcolorbox}}

\newcommand{\theorem}[2]{\custombox{Theorem #1}{violet}{#2}}
\newcommand{\definition}[2]{\custombox{Definition #1}{red}{#2}}
\newcommand{\prop}[2]{\custombox{Proposition #1}{black!50!orange!30!yellow}{#2}}
\newcommand{\other}[2]{\custombox{#1}{green!60!black}{#2}}
\newcommand{\lemma}[2]{\custombox{Lemma #1}{blue!60!black}{#2}}
\newcommand{\corollary}[2]{\custombox{Corollary #1}{orange}{#2}}


\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\R^n}
\newcommand{\Rm}{\R^m}
\newcommand{\Rp}{\R^p}


\begin{document}
\begin{multicols*}{3}
    \begin{center}
        \Large{\textbf{Analysis II HS21}} \\
        \small{by plaffranchi}
    \end{center}
    \section{ODE (ordinary differential equation)}
    \theorem{2.1.6}{Let $F: \mathbb{R}^{2} \rightarrow \mathbb{R}$ be differentiable. Let $x_{0} \in \mathbb{R}$ and $y_{0} \in \mathbb{R}^{2}$. Then the ODE
    $ y^{\prime}=F(x, y) $ has a \textbf{unique solution} $f$ defined on a "largest" open interval I containing $x_{0}$ such that $f\left(x_{0}\right)=y_{0} .$}
    \definition{2.2.1}{Let $I \subset \mathbb{R}$ be an open interval and $k \in \mathbb{N}_0$. An \textbf{homogeneous linear ODE of order} $k$ on $I$ is of the form
    $
        y^{(k)}+a_{k-1} y^{(k-1)}+\cdots+a_{1} y^{\prime}+a_{0} y=0
    $
    where the coefficients $a_{0}, \ldots, a_{k-1}$ are complex-valued functions on $I$, and the unknown is a function $I \to \mathbb{C}$ that is $k$-times differentiable on $I$.
    An equation of the form
    $
        y^{(k)}+a_{k-1} y^{(k-1)}+\cdots+a_{1} y^{\prime}+a_{0} y=b,
    $
    where $b: I \rightarrow \mathbb{C}$ is another function, is called an \textbf{inhomogeneous linear ODE}.}
    \other{Recognize an ODE}{\begin{enumerate}
            \item no coefficients before the highest derivative
            \item all coefficients are continuous
            \item no products of $y$ or their derivatives
            \item no powers of $y$ or their derivatives
            \item no functions depending on $y$ or their derivatives
        \end{enumerate}}

    % Theorem 2.2.3?


    \prop{2.3.1}{Any solution of $y'+ay=0$ is of the form $f(x)=z\exp(-A(x))$ where $A$ is a primitive of $a$. The unique solution with $f(x_0)=y_0$ is $f(x)= y_0\exp(A(x_0)-A(x))$.}


    \other{Solving inhomogeneous equations}{\textbf{Case 1:} Make a guess. For example $y'=y+x^2$ guess $f(x) = ax^2+bx+c$, and solve the equation.\\
        \textbf{Case 2:} Use the variation of the constant. Assume $f_p=z(x)\exp(-A(x))$ for $z: I\to \mathbb{C}$. Then $z'(x) = b(x)\exp(A(x)) \implies k(x) = \int b(x)\exp(A(x))dx$.}


    \definition{Linear differential equations with constant coefficients}{Let $k \in \mathbb{N}_0$, $a_0,...,a_{k-1} \in \mathbb{C}$ fixed and $b$ a general continuous function, then $y^{(k)}+a_{k-1} y^{(k-1)}+\cdots+a_{1} y^{\prime}+a_{0} y=b$ is such equation.}


    \other{Solution of hom. diff. eq. with constant coefficients}{Look for solutions of the form $f(x)=e^{\alpha x}$ for $\alpha \in \mathbb{C}$. Then we have $f^{(j)}(x)=\alpha^{j} e^{\alpha x}$ for all $j \geqslant 0$ and for all $x$, which means that
    \begin{align*}
          & f^{(k)}(x)+a_{k-1} f^{(k-1)}(x)+\cdots+a_{1} f^{\prime}(x)+a_{0} f(x)                \\
        = & e^{\alpha x}\left(\alpha^{k}+a_{k-1} \alpha^{k-1}+\cdots+a_{1} \alpha+a_{0}\right) .
    \end{align*}
    This translates into finding the zeros of the characteristic polynomial:
    \begin{align*}
        P(X)= & X^{k}+a_{k-1} X^{k}+\cdots+a_{1} X+a_{0}                      \\
        =     & \left(X-\alpha_{1}\right) \cdots\left(X-\alpha_{k}\right) = 0
    \end{align*} }


    \other{Imaginary roots}{If a root is not real i.e. $\alpha = \beta + i\gamma$, the solution $f(x)=e^{\alpha x}$ does not take real values, but $\overline{\alpha} = \beta - i\gamma$ is also a root, hence we can write $\widetilde{f}_{1}(x)=e^{\beta x} \cos (\gamma x), \quad \widetilde{f}_{2}(x)=e^{\beta x} \sin (\gamma x)$ instead of $f_{1}(x)=e^{\alpha x}, \quad f_{2}(x)=e^{\bar{\alpha} x}$}


    \other{Multiple roots}{\textbf{Case 1: no multiple roots.} Any solution of the equation is of the form $f(x) = z_1e^{a_1x}+\cdots +z_ke^{a_kx}$.\\
    \textbf{Case 2: multiple roots.}  Suppose that $\alpha$ is a multiple root of order $j$ with $2 \leqslant j \leqslant k$. Then the $k$ functions
    $
        f_{\alpha, 0}(x)=e^{\alpha x}, \quad f_{\alpha, 1}(x)=x e^{\alpha x}, \quad \cdots, \quad f_{\alpha, j-1}(x)=x^{j-1} e^{\alpha x}
    $
    are linearly independent solutions. Taking the union of the functions $f_{\alpha, j}$ for all roots of $P$, each with its multiplicity, gives a basis of the space of solutions.}

    \section{Differential calculus in $\mathbb{R}^n$}


    \definition{3.2.1.}{Let $\left(x_{k}\right)_{k \in \mathbb{N}}$ where $x_{k} \in \mathbb{R}^{n}$. Write
    $
        x_{k}=\left(x_{k, 1}, \ldots, x_{k, n}\right) .
    $
    Let $y=\left(y_{1}, \ldots, y_{n}\right) \in \mathbb{R}^{n}$. The sequence $\left(x_{k}\right)$ \textbf{converges to ($\to$)} $y$ as $k \rightarrow+\infty$ if  $\forall \varepsilon>0$, if $\exists N \geqslant 1$ such that  $\forall n \geqslant N$, we have
    $
        \left\|x_{k}-y\right\|<\varepsilon \text {. }
    $}

    \lemma{3.2.2.}{
    $\left(x_{k}\right) \to y$ as $k \rightarrow+\infty$ $\iff$ either:
    (1) $\forall i, 1 \leqslant i \leqslant n$, the sequence of real numbers $\left(x_{k, i}\right) \to y_{i}$.
    (2) The sequence of real numbers $\left\|x_{k}-y\right\|\to 0$ as $k \rightarrow+\infty$.}


    \definition{3.2.3.}{ Let $X \subset \mathbb{R}^{n}$ and $f: X \rightarrow \mathbb{R}^{m}$. (1) Let $x_{0} \in X$. $f$ is\textbf{ continuous at $x_{0}$} if $\forall \varepsilon>0\ \exists \delta>0$ s.t. $\left\|x-x_{0}\right\|<\delta \implies
        \left\|f(x)-f\left(x_{0}\right)\right\|<\varepsilon$, $\forall x \in X$. (2) $f$ is \textbf{continuous} on $X$ if it is continuous at $x_{0}$ $\forall x_{0} \in X$.}


    \prop{3.2.4.}{Let $X \subset \mathbb{R}^{n}$ and $f: X \rightarrow \mathbb{R}^{m}$. Let $x_{0} \in X$. The function $f$ is continuous at $x_{0} \iff \forall \left(x_{k}\right)_{k} \geqslant 1$ in $X$ s.t. $x_{k} \rightarrow x_{0}$ as $k \rightarrow+\infty$, the sequence $\left(f\left(x_{k}\right)\right)_{k} \geqslant 1$ in $\mathbb{R}^{m}$ converges to $f(x)$.}


    \definition{3.2.5.}{Let $X \subset \mathbb{R}^{n}$ and $f: X \rightarrow \mathbb{R}^{m}$. Let $x_{0} \in X$ and $y \in \mathbb{R}^{m}$. We say that $f$ has the \textbf{limit} $y$ as $x \rightarrow x_{0}$ with $x \neq x_{0}$ if for every $\varepsilon>0$, there exists $\delta>0$, s.t.  $\forall x \in X, x \neq x_{0}$, s.t. $\left\|x-x_{0}\right\|<\delta$, we have $\|f(x)-y\|<\varepsilon$. We then write
    $
        \lim _{x \rightarrow x_{0} \atop x \neq x_{0}} f(x)=y
    $.}


    \prop{3.2.7.}{Let $X \subset \mathbb{R}^{n}$ and $f: X \rightarrow \mathbb{R}^{m} .$ Let $x_{0} \in X$ and $y \in \mathbb{R}^{m} .$ We have
        $
            \lim _{x \rightarrow x_{0} \atop x \neq x_{0}} f(x)=y \iff
        $
        $\forall\left(x_{k}\right) \in X$ s.t. $x_{k} \rightarrow x$ as $k \rightarrow+\infty$, and $x_{k} \neq x_{0}$, the sequence $\left(f\left(x_{k}\right)\right)$ in $\mathbb{R}^{m}$ converges to $y$.}


    \prop{3.2.9.}{Let $X \subset \mathbb{R}^{n}, Y \subset \mathbb{R}^{m}$ and $p \geqslant 1$ an integer. Let $f: X \rightarrow Y$ and $g: Y \rightarrow \mathbb{R}^{p}$ be continuous functions. Then the composite $g \circ f$ is continuous.}

    \definition{3.2.11.}{
        (1) A subset $X \subset \mathbb{R}^{n}$ is \textbf{bounded} if the set of $\|x\|$ for $x \in X$ is bounded in $\mathbb{R}$.
        (2) A subset $X \subset \mathbb{R}^{n}$ is \textbf{closed} if for every sequence $\left(x_{k}\right)$ in $X$ that converges in $\mathbb{R}^{n}$ to some vector $y \in \mathbb{R}^{n}$, we have $y \in X$.
        (3) A subset $X \subset \mathbb{R}^{n}$ is \textbf{compact} if it is bounded and closed.
    }

    \prop{3.2.13.}{Let $g:\Rn \to \Rm$ be a continuous map. For any closed set $Y\subset \Rm$, $f^{-1}(Y) = {x\in\Rn : f(x)\in Y} \subset \Rn$ is closed.}

    \theorem{3.2.15.}{Let $X\subset \Rn$ be a non-empty compact set and $f:X\to\R$ a continuous function. Then $f$ is bounded and achieves its max and min. I.e. $\exists x_+,x_-\in X$ s.t. $f(x_+)=\sup_{x\in X}f(x)$, $f(x_-)=\inf_{x\in X}f(x)$.}

    \definition{3.3.1.}{A subset $X \subset \Rn$ is \textbf{open} if, for any $x=\left(x_{1}, \ldots, x_{n}\right) \in X$, there exists $\delta>0$ such that the set
    $$
        \left\{y=\left(y_{1}, \ldots, y_{n}\right) \in \Rn:\left|x_{i}-y_{i}\right|<\delta \text { for all } i\right\}
    $$
    is contained in $X$.
    In other words: any point of $\Rn$ obtained by changing any coordinate of $x$ by at most $\delta$ is still in $X$.}

    \prop{3.3.2.}{A set $X \subset \Rn$ is open if and only if the complement
        $
            Y=\left\{x \in \Rn: x \notin X\right\}
        $
        is closed.}

    \corollary{3.3.3.}{If $f: \Rn \rightarrow \Rm$ is continuous and $Y \subset \Rm$ is open, then $f^{-1}(Y)$ is open in $\Rn$.}

    \definition{3.3.5}{Let $X \subset \Rn$ be an open set. Let $f: X \rightarrow \Rm$ be a function. Let $1 \leqslant i \leqslant n$. We say that $f$ has a \textbf{partial derivative} on $X$ with respect to the $i$-th variable, or coordinate, if for all $x_{0}=\left(x_{0,1}, \ldots, x_{0, n}\right) \in X$, the function defined by
    $$
        g(t)=f\left(x_{0,1}, \ldots, x_{0, i-1}, t, x_{0, i+1}, \ldots, x_{0, n}\right)
    $$
    on the set
    $
        I=\left\{t \in \R:\left(x_{0,1}, \ldots, x_{0, i-1}, t, x_{0, i+1}, \ldots, x_{0, n}\right) \in X\right\}
    $
    is differentiable at $t=x_{0, i}$. Its \textbf{derivative} $g^{\prime}\left(x_{0, i}\right)$ at $x_{0, i}$ is denoted
    $$
        \frac{\partial f}{\partial x_{i}}\left(x_{0}\right), \quad \partial_{x_{i}} f\left(x_{0}\right), \quad \partial_{i} f\left(x_{0}\right)
    $$}

    \prop{3.3.7.}{Consider $X \subset \Rn$ open and $f, g$ functions from $X$ to $\Rm$. Let $1 \leqslant i \leqslant n$.
        (1) If $f$ and $g$ have partial derivatives with respect to the $i$-th coordinate on $X$, then $f+g$ also does, and
        $$
            \partial_{x_{i}}(f+g)=\partial_{x_{i}}(f)+\partial_{x_{i}}(g) .
        $$
        (2) If $m=1$, and if $f$ and $g$ have partial derivatives with respect to the $i$-th coordinate on $X$, then $f g$ also does and
        $$
            \partial_{x_{i}}(f g)=\partial_{x_{i}}(f) g+f \partial_{x_{i}}(g) .
        $$
        Furthermore, if $g(x) \neq 0$ for all $x \in X$, then $f / g$ has a partial derivative with respect to the $i$-th coordinate on $X$, with
        $$
            \partial_{x_{i}}(f / g)=\left(\partial_{x_{i}}(f) g-f \partial_{x_{i}}(g)\right) / g^{2} .
        $$}

    \definition{3.3.9. (Jacobi matrix)}{Let $X \subset \Rn$ open and $f: X \rightarrow \Rm$ a function with partial derivatives on $X$. Write
        $$
            f(x)=\left(f_{1}(x), \ldots, f_{m}(x)\right) .
        $$
        For any $x \in X$, the matrix
        $$
            J_{f}(x)=\left(\partial_{x_{j}} f_{i}(x)\right)_{1 \leqslant i \leqslant m \atop 1 \leqslant j \leqslant n}
        $$
        with $m$ rows and $n$ columns is called the \textbf{Jacobi matrix} of $f$ at $x$.}

    \definition{3.3.11 (Gradient, Divergence)}{Let $X \subset \Rn$ be open.
        (1) Let $f: X \rightarrow \R$ be a function. If all partial derivatives of $f$ exist at $x_{0} \in X$, then the column vector
        $$
            \left(\begin{array}{c}
                    \partial_{x_{1}} f\left(x_{0}\right) \\
                    \cdots                               \\
                    \partial_{x_{n}} f\left(x_{0}\right)
                \end{array}\right)
        $$
        is called the \textbf{gradient} at $x_{0}$, and is denoted $\nabla f\left(x_{0}\right)$.
        (2) Let $f=\left(f_{1}, \ldots, f_{n}\right): X \rightarrow \Rn$ be a function with values in $\Rn$ such that all partial derivatives of all coordinates $f_{i}$ of $f$ exist at $x_{0} \in X$. Then the real number
        $$
            \operatorname{Tr}\left(J_{f}\left(x_{0}\right)\right)=\sum_{i=1}^{n} \partial_{x_{i}} f_{i}\left(x_{0}\right),
        $$
        the trace of the Jacobi matrix, is called the \textbf{divergence} of $f$ at $x_{0}$, and is denoted $\operatorname{div}(f)\left(x_{0}\right)$.}

    \definition{3.4.2.}{Let $X \subset \Rn$ be open and $f: X \rightarrow \Rm$ be a function. Let $u$ be a linear map $\Rn \rightarrow \Rm$ and $x_{0} \in X$. We say that \textbf{$f$ is differentiable at $x_{0}$ with differential $u$} if
    $$
        \lim _{x \rightarrow x_{0}} \frac{1}{\left\|x-x_{0}\right\|}\left(f(x)-f\left(x_{0}\right)-u\left(x-x_{0}\right)\right)=0
    $$
    where the limit is in $\Rm$. We then denote $d f\left(x_{0}\right)=u$.
    If $f$ is differentiable at every $x_{0} \in X$, then we say that $f$ is differentiable on $X$.}

    \prop{3.4.4.}{Let $X \subset \Rn$ be open and $f: X \rightarrow \Rm$ be a function that is differentiable on $X$.
    (1) The function $f$ is continuous on $X$.
    (2) The function $f$ admits partial derivatives on $X$ with respect to each variable.
    (3) Assume that $m=1$. Let $x_{0} \in X$, and let
    $
        u\left(x_{1}, \ldots, x_{n}\right)=a_{1} x_{1}+\cdots+a_{n} x_{n}
    $
    be the differential of $f$ at $x_{0}$. We then have
    $
        \partial_{x_{i}} f\left(x_{0}\right)=a_{i}
    $
    for $1 \leqslant i \leqslant n$.}


    \prop{3.4.6.}{Let $X \subset \Rn$ be open, $f: X \rightarrow \Rm$ and $g: X \rightarrow \Rm$ differentiable functions on $X$.
        (1) The function $f+g$ is differentiable with differential $d(f+g)=d f+d g$, and if $m=1$, then $f g$ is differentiable.
        (2) If $m=1$ and if $g(x) \neq 0$ for all $x \in X$, then $f / g$ is differentiable.}


    \prop{3.4.7.}{Let $X \subset \Rn$ be open, $f: X \rightarrow \Rm$ a function on $X .$ If $f$ has all partial derivatives on $X$, and if the partial derivatives of $f$ are continuous on $X$, then $f$ is differentiable on $X$, with differential determined by its partial derivatives, in the sense that the matrix of the differential $d f\left(x_{0}\right)$, with respect to the canonical basis of $\Rn$ and $\Rm$, is the Jacobi matrix of $f$ at $x_{0}$.}

    \prop{3.4.9 (Chain rule)}{Let $X \subset \Rn$ be open, $Y \subset \Rm$ be open, and let $f: X \rightarrow Y$ and $g: Y \rightarrow \Rp$ be differentiable functions. Then $g \circ f: X \rightarrow \Rp$ is differentiable on $X$, and for any $x \in X$, its differential is given by the composition
    $$
        d(g \circ f)\left(x_{0}\right)=d g\left(f\left(x_{0}\right)\right) \circ d f\left(x_{0}\right) \text {. }
    $$
    In particular, the Jacobi matrix satisfies
    $$
        J_{g \circ f}\left(x_{0}\right)=J_{g}\left(f\left(x_{0}\right)\right) J_{f}\left(x_{0}\right)
    $$
    where the right-hand side is a matrix product.}

    \definition{3.4.11.}{Let $X \subset \Rn$ be open and $f: X \rightarrow \Rm$ a function that is differentiable. Let $x_{0} \in X$ and $u=d f\left(x_{0}\right)$ be the differential of $f$ at $x_{0}$. The graph of the affine linear approximation
        $$
            g(x)=f\left(x_{0}\right)+u\left(x-x_{0}\right)
        $$
        from $\Rn$ to $\Rm$, or in other words the set
        $$
            \left\{(x, y) \in \Rn \times \Rm: y=f\left(x_{0}\right)+u\left(x-x_{0}\right)\right.
        $$
        is called the \textbf{tangent space} at $x_{0}$ to the graph of $f$.}

    \definition{3.4.13.}{Let $X \subset \Rn$ be an open set and let $f: X \rightarrow \Rm$ be a function. Let $v \in \Rn$ be a non-zero vector and $x_{0} \in X$. We say that $f$ has \textbf{directional derivative $w \in \Rm$ in the direction $v$}, if the function $g$ defined on the set
        $$
            I=\left\{t \in \R: x_{0}+t v \in X\right\}
        $$
        by
        $$
            g(t)=f\left(x_{0}+t v\right)
        $$
        has a derivative at $t=0$, and this is equal to $w$.
        In other words, this means that the limit
        $$
            \lim _{t \rightarrow 0 \atop t \neq 0} \frac{f\left(x_{0}+t v\right)-f\left(x_{0}\right)}{t}
        $$
        exists and is equal to $w$.}


    \prop{3.4.15. }{Let $X \subset \Rn$ be an open set and let $f: X \rightarrow \Rm$ be a differentiable function. Then for any $x \in X$ and non-zero $v \in \Rn$, the function $f$ has a directional derivative at $x_{0}$ in the direction $v$, equal to df $\left(x_{0}\right)(v)$.}

    \definition{3.5.1.}{Let $X \subset \Rn$ be open and $f: X \rightarrow \Rm$.
        We say that $f$ is of class $C^{1}$ if $f$ is differentiable on $X$ and all its partial derivatives are continuous. The set of functions of class $C^{1}$ from $X$ to $\Rm$ is denoted $C^{1}\left(X ; \Rm\right)$.
        Let $k \geqslant 2$. We say, by induction, that $f$ is of class $C^{k}$ if it is differentiable and each partial derivative $\partial_{x_{i}} f: X \rightarrow \Rm$ is of class $C^{k-1}$. The set of functions of class $C^{k}$ from $X$ to $\Rm$ is denoted $C^{k}\left(X ; \Rm\right)$.

        If $f \in C^{k}\left(X ; \Rm\right)$ for all $k \geqslant 1$, then we say that $f$ is of class $C^{\infty}$. The set of such functions is denoted $C^{\infty}\left(X ; \Rm\right)$.}

    \prop{3.5.4 (Mixed derivatives commute)}{
        $k \geqslant 2$. Let $X \subset \Rn$ be open and let $f: X \rightarrow \R^{m}$ be a function of class $C^{k}$. Then the partial derivatives of order $k$ are independent of the order in which the partial derivatives are taken: for any variables $x$ and $y$, we have
        $
            \partial_{x, y} f=\partial_{y, x} f
        $
        and for any variables $x, y, z$, we have
        $$
            \partial_{x, y, z} f=\partial_{x, z, y} f=\partial_{y, z, x} f=\partial_{z, x, y} f=\cdots
        $$
    }

    \definition{3.5.9 (Hessian).}{Let $X \subset \Rn$ be open and $f: X \rightarrow \R$ a $C^{2}$ function. For $x \in X$, the \textbf{Hessian matrix} of $f$ at $x$ is the symmetric square matrix
        $$
            \operatorname{Hess}_{f}(x)=\left(\partial_{x_{i}, x_{j}} f\right)_{1 \leqslant i, j \leqslant n} .
        $$
        We also sometimes write simply $H_{f}(x)$.}

    \definition{3.7.1 (Taylor polynomials).}{Let $k \geqslant 1$ be an integer. Let $f: X \rightarrow \R$ be a function of class $C^{k}$ on $X$, and fix $x_{0} \in X$. The $k$-th Taylor polynomial of $f$ at the point $x_{0}$ is the polynomial in $n$ variables of degree $\leqslant k$ given by
        $$
            \begin{aligned}
                T_{k} f\left(y ; x_{0}\right)= & f\left(x_{0}\right) +\sum_{i=1}^{n} \frac{\partial f}{\partial x_{i}}\left(x_{0}\right) y_{i}+\cdots                                                                                           \\
                                               & +\sum_{m_{1}+\cdots+m_{n}=k} \frac{1}{m_{1} ! \cdots m_{n} !} \frac{\partial^{k} f}{\partial x_{1}^{m_{1}} \cdots \partial x_{n}^{m_{n}}}\left(x_{0}\right) y_{1}^{m_{1}} \cdots y_{n}^{m_{n}}
            \end{aligned}
        $$
        where the last sum ranges over the tuples of $n$ non-negative integers such that the sum is $k$.}

    \prop{3.7.3 (Taylor approximation)}{Let $k \geqslant 1$ be an integer. Let $X \subset \Rn$ be open and $f: X \rightarrow \R$ be a function of class $C^{k}$. For $x_{0}$ in $X$, if we define $E_{k} f\left(x ; x_{0}\right)$ by
    $$
        f(x)=T_{k} f\left(x-x_{0} ; x_{0}\right)+E_{k} f\left(x ; x_{0}\right)
    $$
    then we have
    $$
        \lim _{x \rightarrow x_{0} \atop x \neq x_{0}} \frac{E_{k} f\left(x ; x_{0}\right)}{\left\|x-x_{0}\right\|^{k}}=0
    $$}

    \prop{3.8.1.}{Let $X \subset \Rn$ be open and $f: X \rightarrow \R$ a differentiable function. If $x_{0} \in X$ is such that
        $f(y) \leqslant f\left(x_{0}\right)$ for all $y$ close enough to $x_{0}$ (local maximum at $x_{0}$ )
        or $f(y) \geqslant f\left(x_{0}\right)$ for all $y$ close enough to $x_{0}$ (local minimum at $x_{0}$ ).
        Then we have $d f\left(x_{0}\right)=0$, or in other words $\nabla f\left(x_{0}\right)=0$, or equivalently
        $
            \frac{\partial f}{\partial x_{i}}\left(x_{0}\right)=0
        $
        for $1 \leqslant i \leqslant n$.}


    \definition{3.8.2 (Critical point)}{Let $X \subset \Rn$ be open and $f: X \rightarrow \R$ a differentiable function. A point $x_{0} \in X$ such that $\nabla f\left(x_{0}\right)=0$ is called a \textbf{critical point} of the function $f$.}

    \definition{3.8.6 (Non-degenerate critical point)}{Let $X \subset \Rn$ be open and $f: X \rightarrow$ $\Rn$ a function of class $C^{2}$. A critical point $x_{0} \in X$ of $f$ is called \textbf{non-degenerate} if the Hessian matrix has non-zero determinant.}


    \corollary{3.8}{
        Let $X \subset \Rn$ be open and $f: X \rightarrow \R$ a function of class $C^{2}$. Let $x_{0}$ be a non-degenerate critical point of $f .$ Let $p$ and $q$ be the number of positive and negative eigenvalues of Hess $_{f}\left(x_{0}\right)$. (1) If $p=n$, equivalently if $q=0$, the function $f$ has a local minimum at $x_{0}$. (2) If $q=n$, equivalently if $p=0$, the function $f$ has a local maximum at $x_{0}$. (3) Otherwise, equivalently if $p q \neq 0$, the function $f$ does not have a local extremum at $x_{0}$. One then says that $f$ has a saddle point at $x_{0}$.
    }

    \prop{3.9.2}{
        Let $X \subset \Rn$ be open and let $f: X \rightarrow \R$ and $g: X \rightarrow \R$ be functions of class $C^{1}$. If $x_{0} \in X$ is a local extremum of the function $f$ restricted to the set
        $
            Y=\{x \in X: g(x)=0\}
        $
        then either $\nabla g\left(x_{0}\right)=0$, or there exists $\lambda_{0} \in \R$ such that
        $$
            \left\{\begin{array}{l}
                \nabla f\left(x_{0}\right)=\lambda \nabla g\left(x_{0}\right) \\
                g\left(x_{0}\right)=0
            \end{array}\right.
        $$
        or in other words, there exists $\lambda$ such that $\left(x_{0}, \lambda\right)$ is a critical point of the differentiable function $h: X \times \R \rightarrow \R$ defined by
        $
            h(x, \lambda)=f(x)-\lambda g(x) .
        $
        Such a value $\lambda$ is called a Lagrange multiplier at $x_{0}$.
    }

    \definition{3.10.1 (Change of variable)}{
    Let $X \subset \Rn$ be open and $f: X \rightarrow \Rn$ be differentiable. Let $x_{0} \in X$. We say that $f$ is a \textbf{change of variable} around $x_{0}$ if there is a radius $r>0$ such that the restriction of $f$ to the ball
    $$
        B=\left\{x \in \Rn:\left\|x-x_{0}\right\|<r\right\}
    $$
    of radius $r$ around $x_{0}$ has the property that the image $Y=f(B)$ is open in $\Rn$, and if there is a differentiable map $g: Y \rightarrow B$ such that $f \circ g=\operatorname{Id}_{Y}$ and $g \circ f=\operatorname{Id}_{B}$.
    }

    \theorem{3.10.2 (Inverse funtion theorem)} {
    (Inverse function theorem). Let $X \subset \Rn$ be open and $f: X \rightarrow \Rn$
    differentiable. If $x_{0} \in X$ is such that $\operatorname{det}\left(J_{f}\left(x_{0}\right)\right) \neq 0$, i.e., such that the Jacobian
    trix of $f$ at $x_{0}$ is invertible, then $f$ is a change of variable around $x_{0}$. Moreover, the Jacobian of $g$ at $x_{0}$ is determined by
    $$
        J_{g}\left(f\left(x_{0}\right)\right)=J_{f}\left(x_{0}\right)^{-1}
    $$
    In addition, if $f$ is of class $C^k$, then $g$ is of class $C^k$.
    }

    \theorem{3.10.4 (Implicit Function Theorem).}{Let $X \subset \R^{n+1}$ be open and let $g: X \rightarrow \R$ be of class $C^{k}$ with $k \geqslant 1$. Let $\left(x_{0}, y_{0}\right) \in \Rn \times \R$ be such that $g\left(x_{0}, y_{0}\right)=0$. Assume that
    $$
        \partial_{y} g\left(x_{0}, y_{0}\right) \neq 0
    $$
    Then there exists an open set $U \subset \Rn$ containing $x_{0}$, an open interval $I \subset \R$ containing $y_{0}$, and a function $f: U \rightarrow \R$ of class $C^{k}$ such that the system of equations
    $$
        \left\{\begin{array}{l}
            g(x, y)=0 \\
            x \in U, \quad y \in I
        \end{array}\right.
    $$
    is equivalent with $y=f(x)$. In particular, $f\left(x_{0}\right)=y_{0}$. Moreover, the gradient of $f$ at $x_{0}$ is given by
    $$
        \nabla f\left(x_{0}\right)=-\frac{1}{\left(\partial_{y} g\right)\left(x_{0}, y_{0}\right)} \nabla_{x} g\left(x_{0}, y_{0}\right)
    $$
    where $\nabla_{x} g=\left(\partial_{x_{1}} g, \ldots, \partial_{x_{n}} g\right)$}

    \section{Integration in $\Rn$}

    \definition{4.1.1. (parameterized curve, line integral)}{(1) Let $I=[a, b]$ be a closed and bounded interval in $\R$. Let $f(t)=\left(f_{1}(t), \ldots, f_{n}(t)\right)$
    be a continuous function from $I$ to $\Rn$, i.e., $f_{i}$ is continuous for $1 \leqslant i \leqslant n$. Then we define
    $$
        \int_{a}^{b} f(t) d t=\left(\int_{a}^{b} f_{1}(t), \ldots, \int_{a}^{b} f_{n}(t) d t\right) \in \Rn .
    $$
    (2) A \textbf{parameterized curve} in $\Rn$ is a continuous map $\gamma:[a, b] \rightarrow \Rn$ that is piecewise $C^{1}$, i.e., there exists $k \geqslant 1$ and a partition
    $$
        a=t_{0}<t_{1}<\cdots<t_{k-1}<t_{k}=b
    $$
    such that the restriction of $f$ to $] t_{j-1}, t_{j}\left[\right.$ is $C^{1}$ for $1 \leqslant j \leqslant k$. We say that $\gamma$ is a parameterized curve, or a path $\mathrm{x}$, between $\gamma(a)$ and $\gamma(b)$.
    (3) Let $\gamma:[a, b] \rightarrow \Rn$ be a parameterized curve. Let $X \subset \Rn$ be a subset containing the image of $\gamma$, and let $f: X \rightarrow \Rn$ be a continuous function. The integral
    $$
        \int_{a}^{b} f(\gamma(t)) \cdot \gamma^{\prime}(t) d t \in \R
    $$
    is called the \textbf{line integral of $f$ along $\gamma$}. It is denoted
    $$
        \int_{\gamma} f(s) \cdot d s, \quad \text { or } \quad \int_{\gamma} f(s) \cdot d \vec{s} .
    $$
    }

    \definition{4.1.4}{
    Let $\gamma:[a, b] \rightarrow \Rn$ be a parameterized curve. An \textbf{oriented reparameterization} of $\gamma$ is a parameterized curve $\sigma:[c, d] \rightarrow \Rn$ such that $\sigma=\gamma \circ \varphi$, where $\varphi:[c, d] \rightarrow[a, b]$ is a continuous map, differentiable on $] a, b[$, that is strictly increasing and satisfies $\varphi(a)=c$ and $\varphi(b)=d$.
    }

    \prop{4.1.5.}{Let $\gamma$ be a parameterized curve in $\Rn$ and $\sigma$ an oriented reparameterization of $\gamma$. Let $X$ be a set containing the image of $\gamma$, or equivalently the image of $\sigma$, and $f: X \rightarrow \Rn$ a continuous function. Then we have
        $$
            \int_{\gamma} f(s) \cdot d \vec{s}=\int_{\sigma} f(s) \cdot d \vec{s} .
        $$}

    \definition{4.1.8}{
        Let $X \subset \Rn$ and $f: X \rightarrow \Rn$ a continuous vector field. If, for any $x_{1}, x_{2}$ in $X$, the line integral
        $
            \int_{\gamma} f(s) \cdot d \vec{s}
        $
        is independent of the choice of a parameterized curve $\gamma$ in $X$ from $x_{1}$ to $x_{2}$, then we say that the vector field is conservative.
    }

    \theorem{4.1.10}{
    Let $X$ be an open set and $f$ a conservative vector field. Then there exists a $C^{1}$ function $g$ on $X$ such that $f=\nabla g$.

    If any two points of $X$ can be joined by a parameterized curve, then $g$ is unique up to addition of a constant: if $\nabla g_{1}=f$, then $g-g_{1}$ is constant on $X$.
    }

    \prop{4.1.13}{
        Let $X \subset \Rn$ be an open set and $f: X \rightarrow \Rn$ a vector field of class $C^{1}$. Write
        $
            f(x)=\left(f_{1}(x), \ldots, f_{n}(x)\right) .
        $
        If $f$ is conservative, then we have
        $$
            \frac{\partial f_{i}}{\partial x_{j}}=\frac{\partial f_{j}}{\partial x_{i}}
        $$
        for any integers with $1 \leqslant i \neq j \leqslant n$.
    }

    \definition{4.1.15 (start shaped)}{A subset $X\subset\Rn$ is \textbf{star shaped} if there exists $x_0 \in X$ such that, for alla $x\in X$, the line segment joining $x_0$ to $x$ is contained in $X$. We then also say that $X$ is \textbf{star-shaped around} $x_0$}

    \theorem{4.1.17}{Let $X$ be a star-shaped open subset of $\Rn$. Let $f$ be a $C^1$ vector field s.t. $\frac{\partial f_{i}}{\partial x_{j}}=\frac{\partial f_{j}}{\partial x_{i}}$ on $X$ for all $i\neq j$ between 1 and $n$. Then the vector field $f$ is conservative.}

    \definition{4.1.20 (curl)}{
        Definition 4.1.20. Let $X \subset \R^{3}$ be an open set and $f: X \rightarrow \R^{3}$ a $C^{1}$ vector field. Then the curl of $f$, denoted $\operatorname{curl}(f)$, is the continuous vector field on $X$ defined by
        $$
            \operatorname{curl}(f)=\left(\begin{array}{c}
                    \partial_{y} f_{3}-\partial_{z} f_{2} \\
                    \partial_{z} f_{1}-\partial_{x} f_{3} \\
                    \partial_{x} f_{2}-\partial_{y} f_{1}
                \end{array}\right)
        $$
        where $f(x, y, z)=\left(f_{1}(x, y, z), f_{2}(x, y, z), f_{3}(x, y, z)\right)$.
    }


\end{multicols*}



\end{document}